{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all modules as dataset instead (w/weights to balance out different amt of students for each module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id_student code_module  score Gender                Region  \\\n",
      "0        11391         AAA   82.0      M   East Anglian Region   \n",
      "1        28400         AAA   66.4      F              Scotland   \n",
      "3        31604         AAA   76.0      F     South East Region   \n",
      "4        32885         AAA   54.4      F  West Midlands Region   \n",
      "5        38053         AAA   68.0      M                 Wales   \n",
      "6        45462         AAA   68.0      M              Scotland   \n",
      "7        45642         AAA   72.4      F  North Western Region   \n",
      "8        52130         AAA   71.4      F   East Anglian Region   \n",
      "9        53025         AAA   78.0      M          North Region   \n",
      "10       57506         AAA   74.0      M          South Region   \n",
      "\n",
      "                            HLE Age group  Credit Distribution  \\\n",
      "0              HE Qualification      55<=                  240   \n",
      "1              HE Qualification     35-55                   60   \n",
      "3         A Level or Equivalent     35-55                   60   \n",
      "4            Lower Than A Level      0-35                   60   \n",
      "5         A Level or Equivalent     35-55                   60   \n",
      "6              HE Qualification      0-35                   60   \n",
      "7         A Level or Equivalent      0-35                  120   \n",
      "8         A Level or Equivalent      0-35                   90   \n",
      "9   Post Graduate Qualification      55<=                   60   \n",
      "10           Lower Than A Level     35-55                   60   \n",
      "\n",
      "    assessment_completion_ratio  total_vle_clicks  target  \n",
      "0                      0.833333             934.0       1  \n",
      "1                      0.833333            1435.0       1  \n",
      "3                      0.833333            2158.0       1  \n",
      "4                      0.833333            1034.0       1  \n",
      "5                      0.833333            2445.0       1  \n",
      "6                      0.833333            1492.0       1  \n",
      "7                      0.833333            1428.0       1  \n",
      "8                      0.833333            1894.0       1  \n",
      "9                      0.833333            3158.0       1  \n",
      "10                     0.833333            1319.0       1  \n",
      "Number of rows in df_final: 22437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# ============================\n",
    "# Data Loading and Merging\n",
    "# ============================\n",
    "\n",
    "# 1. Load your CSV files into DataFrames\n",
    "df_assessments = pd.read_csv(\"assessments.csv\")\n",
    "df_courses = pd.read_csv(\"courses.csv\")\n",
    "df_studentAssessment = pd.read_csv(\"studentAssessment.csv\")\n",
    "df_studentInfo = pd.read_csv(\"studentInfo.csv\")\n",
    "df_studentRegistration = pd.read_csv(\"studentRegistration.csv\")\n",
    "df_studentVle = pd.read_csv(\"studentVle.csv\")\n",
    "df_vle = pd.read_csv(\"vle.csv\")\n",
    "\n",
    "# Count how many unique assessments exist for each module + presentation\n",
    "df_course_assess_count = (\n",
    "    df_assessments\n",
    "    .groupby([\"code_module\", \"code_presentation\"])[\"id_assessment\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"id_assessment\": \"total_assessments\"})\n",
    ")\n",
    "\n",
    "# Count how many assessments each student attempted\n",
    "df_attempted_count = (\n",
    "    df_studentAssessment\n",
    "    .groupby(\"id_student\")[\"id_assessment\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"id_assessment\": \"num_assessments_attempted\"})\n",
    ")\n",
    "\n",
    "# Compute average score for each student\n",
    "df_avg_score = (\n",
    "    df_studentAssessment\n",
    "    .groupby(\"id_student\")[\"score\"]\n",
    "    .mean()  # or .sum() if you prefer total score\n",
    "    .reset_index()\n",
    "    .rename(columns={\"score\": \"score\"})\n",
    ")\n",
    "\n",
    "df_vle_clicks = (\n",
    "    df_studentVle\n",
    "    .groupby(\"id_student\")[\"sum_click\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sum_click\": \"total_vle_clicks\"})\n",
    ")\n",
    "\n",
    "# 3.1 Merge total_assessments (on code_module + code_presentation)\n",
    "df_merged = pd.merge(\n",
    "    df_studentInfo,\n",
    "    df_course_assess_count,\n",
    "    on=[\"code_module\", \"code_presentation\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3.2 Merge attempted_assessments & average_score (on id_student)\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    df_attempted_count,\n",
    "    on=\"id_student\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    df_avg_score,\n",
    "    on=\"id_student\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3.3 Merge total_vle_clicks (on id_student)\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    df_vle_clicks,\n",
    "    on=\"id_student\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace NaN with 0 if needed\n",
    "df_merged[\"num_assessments_attempted\"] = df_merged[\"num_assessments_attempted\"].fillna(0)\n",
    "df_merged[\"total_assessments\"] = df_merged[\"total_assessments\"].fillna(0)\n",
    "df_merged[\"score\"] = df_merged[\"score\"].fillna(0)  # if missing, assume 0 or handle differently\n",
    "df_merged[\"total_vle_clicks\"] = df_merged[\"total_vle_clicks\"].fillna(0)\n",
    "\n",
    "# Compute assessment_completion_ratio safely\n",
    "def completion_ratio(row):\n",
    "    if row[\"total_assessments\"] == 0:\n",
    "        return 0\n",
    "    return row[\"num_assessments_attempted\"] / row[\"total_assessments\"]\n",
    "\n",
    "df_merged[\"assessment_completion_ratio\"] = df_merged.apply(completion_ratio, axis=1)\n",
    "\n",
    "# Filter out withdrawn\n",
    "df_merged = df_merged[\n",
    "    (df_merged[\"final_result\"] != \"Withdrawn\") \n",
    "].copy()\n",
    "\n",
    "# Create binary target (Pass/Distinction=1, Fail=0)\n",
    "df_merged[\"target\"] = df_merged[\"final_result\"].apply(\n",
    "    lambda x: 1 if x in [\"Pass\", \"Distinction\"] else 0\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_merged.rename(\n",
    "    columns={\n",
    "        \"highest_education\": \"HLE\",\n",
    "        \"age_band\": \"Age group\",\n",
    "        \"studied_credits\": \"Credit Distribution\",\n",
    "        \"gender\": \"Gender\",\n",
    "        \"region\": \"Region\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Final DataFrame including module\n",
    "# ---------------------------\n",
    "df_final = df_merged[\n",
    "    [\n",
    "        \"id_student\",\n",
    "        \"code_module\",            # <-- include the module feature here\n",
    "        \"score\",\n",
    "        \"Gender\",\n",
    "        \"Region\",\n",
    "        \"HLE\",\n",
    "        \"Age group\",\n",
    "        \"Credit Distribution\",\n",
    "        \"assessment_completion_ratio\",\n",
    "        \"total_vle_clicks\",\n",
    "        \"target\"\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "print(df_final.head(10))\n",
    "print(\"Number of rows in df_final:\", len(df_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8862745098039215\n",
      "Decision Tree Balanced Accuracy: 0.8442547404176867\n",
      "Decision Tree F1 Score: 0.9202898550724637\n",
      "Decision Tree Confusion Matrix:\n",
      " [[1289  474]\n",
      " [ 164 3683]]\n",
      "\n",
      "Random Forest Accuracy: 0.8759358288770054\n",
      "Random Forest Balanced Accuracy: 0.8327218607482078\n",
      "Random Forest F1 Score: 0.9129782445611403\n",
      "Random Forest Confusion Matrix:\n",
      " [[1263  500]\n",
      " [ 196 3651]]\n",
      "Random Forest OOB Score: 0.8773994176026624\n",
      "\n",
      "AdaBoost Accuracy: 0.8770053475935828\n",
      "AdaBoost Balanced Accuracy: 0.8242835243291285\n",
      "AdaBoost F1 Score: 0.9150664697193501\n",
      "AdaBoost Confusion Matrix:\n",
      " [[1203  560]\n",
      " [ 130 3717]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Define features including the module and separate target\n",
    "features = [\n",
    "    'score', 'Gender', 'Region', 'HLE', 'Age group',\n",
    "    'Credit Distribution', 'assessment_completion_ratio', 'total_vle_clicks',\n",
    "    'code_module'  # include module\n",
    "]\n",
    "\n",
    "X = df_final[features]\n",
    "y = df_final['target']\n",
    "\n",
    "# One-hot encode categorical variables (including code_module)\n",
    "X_encoded = pd.get_dummies(\n",
    "    X, \n",
    "    columns=['Gender', 'Region', 'HLE', 'Age group', 'code_module'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Create Sample Weights by Module\n",
    "# -----------------------------------\n",
    "# Compute module counts and assign a weight = 1 / count for each sample\n",
    "module_counts = df_final['code_module'].value_counts()\n",
    "df_final['module_weight'] = df_final['code_module'].map(lambda m: 1.0 / module_counts[m])\n",
    "sample_weights = df_final['module_weight']\n",
    "\n",
    "# -----------------------------------\n",
    "# Split the data into training and test sets\n",
    "# -----------------------------------\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X_encoded, y, sample_weights, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Decision Tree Classifier with class_weight and sample weights\n",
    "# -----------------------------------\n",
    "clf_dt = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    class_weight='balanced',  # addressing class imbalance as well\n",
    "    random_state=42\n",
    ")\n",
    "clf_dt.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_dt = clf_dt.predict(X_test)\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "balanced_accuracy_dt = balanced_accuracy_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"Decision Tree Balanced Accuracy:\", balanced_accuracy_dt)\n",
    "print(\"Decision Tree F1 Score:\", f1_dt)\n",
    "print(\"Decision Tree Confusion Matrix:\\n\", cm_dt)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Random Forest Classifier with class_weight and sample weights\n",
    "# -----------------------------------\n",
    "clf_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "clf_rf.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "balanced_accuracy_rf = balanced_accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "oob_score_rf = clf_rf.oob_score_\n",
    "\n",
    "print(\"\\nRandom Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest Balanced Accuracy:\", balanced_accuracy_rf)\n",
    "print(\"Random Forest F1 Score:\", f1_rf)\n",
    "print(\"Random Forest Confusion Matrix:\\n\", cm_rf)\n",
    "print(\"Random Forest OOB Score:\", oob_score_rf)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. AdaBoost Classifier with base estimator having class_weight and sample weights\n",
    "# -----------------------------------\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=1,\n",
    "    class_weight='balanced',  # base estimator with class_weight\n",
    "    random_state=42\n",
    ")\n",
    "clf_ab = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "clf_ab.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_ab = clf_ab.predict(X_test)\n",
    "\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "balanced_accuracy_ab = balanced_accuracy_score(y_test, y_pred_ab)\n",
    "f1_ab = f1_score(y_test, y_pred_ab)\n",
    "cm_ab = confusion_matrix(y_test, y_pred_ab)\n",
    "\n",
    "print(\"\\nAdaBoost Accuracy:\", accuracy_ab)\n",
    "print(\"AdaBoost Balanced Accuracy:\", balanced_accuracy_ab)\n",
    "print(\"AdaBoost F1 Score:\", f1_ab)\n",
    "print(\"AdaBoost Confusion Matrix:\\n\", cm_ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridsearchCV finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Decision Tree Params: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Decision Tree CV Score: 0.8612072419796716\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best Random Forest Params: {'max_depth': 5, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Random Forest CV Score: 0.8690336673976627\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best AdaBoost Params: {'estimator__max_depth': 3, 'estimator__min_samples_split': 2, 'learning_rate': 0.01, 'n_estimators': 200}\n",
      "Best AdaBoost CV Score: 0.8568870689894412\n",
      "\n",
      "Decision Tree Test Set:\n",
      "Accuracy: 0.8627450980392157\n",
      "Balanced Accuracy: 0.8601302721909405\n",
      "F1 Score: 0.8965331900026875\n",
      "Confusion Matrix:\n",
      " [[1504  259]\n",
      " [ 511 3336]]\n",
      "\n",
      "Random Forest Test Set:\n",
      "Accuracy: 0.8805704099821747\n",
      "Balanced Accuracy: 0.8674428778249613\n",
      "F1 Score: 0.9120273109243697\n",
      "Confusion Matrix:\n",
      " [[1467  296]\n",
      " [ 374 3473]]\n",
      "OOB Score: 0.8795388363938907\n",
      "\n",
      "AdaBoost Test Set:\n",
      "Accuracy: 0.8869875222816399\n",
      "Balanced Accuracy: 0.8529173383330426\n",
      "F1 Score: 0.9197671475575804\n",
      "Confusion Matrix:\n",
      " [[1342  421]\n",
      " [ 213 3634]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Define features including the module and separate target\n",
    "features = [\n",
    "    'score', 'Gender', 'Region', 'HLE', 'Age group',\n",
    "    'Credit Distribution', 'assessment_completion_ratio', 'total_vle_clicks',\n",
    "    'code_module'  # include module\n",
    "]\n",
    "\n",
    "X = df_final[features]\n",
    "y = df_final['target']\n",
    "\n",
    "# One-hot encode categorical variables (including code_module)\n",
    "X_encoded = pd.get_dummies(\n",
    "    X, \n",
    "    columns=['Gender', 'Region', 'HLE', 'Age group', 'code_module'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Create Sample Weights by Module\n",
    "# -----------------------------------\n",
    "# Compute module counts and assign a weight = 1 / count for each sample\n",
    "module_counts = df_final['code_module'].value_counts()\n",
    "df_final['module_weight'] = df_final['code_module'].map(lambda m: 1.0 / module_counts[m])\n",
    "sample_weights = df_final['module_weight']\n",
    "\n",
    "# -----------------------------------\n",
    "# Split the data into training and test sets\n",
    "# -----------------------------------\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X_encoded, y, sample_weights, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ===========\n",
    "# 1. Decision Tree with GridSearchCV\n",
    "# ===========\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='balanced_accuracy',  # or use 'accuracy' or other metric\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "# Pass sample_weight via fit_params\n",
    "grid_search_dt.fit(X_train, y_train, sample_weight=w_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "print(\"Best Decision Tree Params:\", grid_search_dt.best_params_)\n",
    "print(\"Best Decision Tree CV Score:\", grid_search_dt.best_score_)\n",
    "\n",
    "# ===========\n",
    "# 2. Random Forest with GridSearchCV\n",
    "# ===========\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', bootstrap=True, random_state=42, oob_score=True)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train, sample_weight=w_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"Best Random Forest Params:\", grid_search_rf.best_params_)\n",
    "print(\"Best Random Forest CV Score:\", grid_search_rf.best_score_)\n",
    "\n",
    "# ===========\n",
    "# 3. AdaBoost with GridSearchCV\n",
    "# ===========\n",
    "\n",
    "# For AdaBoost, we tune hyperparameters for the AdaBoost itself and its base estimator.\n",
    "# Note: In GridSearchCV, parameters for the base estimator are prefixed with 'estimator__'\n",
    "base_est = DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=1)\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'estimator__max_depth': [1, 2, 3],\n",
    "    'estimator__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=base_est, random_state=42)\n",
    "grid_search_ada = GridSearchCV(\n",
    "    estimator=ada,\n",
    "    param_grid=param_grid_ada,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_ada.fit(X_train, y_train, sample_weight=w_train)\n",
    "best_ada = grid_search_ada.best_estimator_\n",
    "print(\"Best AdaBoost Params:\", grid_search_ada.best_params_)\n",
    "print(\"Best AdaBoost CV Score:\", grid_search_ada.best_score_)\n",
    "\n",
    "# ===========\n",
    "# Evaluation on Test Set (for all models)\n",
    "# ===========\n",
    "\n",
    "# Decision Tree Evaluation\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "balanced_accuracy_dt = balanced_accuracy_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(\"\\nDecision Tree Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_dt)\n",
    "print(\"F1 Score:\", f1_dt)\n",
    "print(\"Confusion Matrix:\\n\", cm_dt)\n",
    "\n",
    "# Random Forest Evaluation\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "balanced_accuracy_rf = balanced_accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "oob_score_rf = best_rf.oob_score_\n",
    "\n",
    "print(\"\\nRandom Forest Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n",
    "print(\"OOB Score:\", oob_score_rf)\n",
    "\n",
    "# AdaBoost Evaluation\n",
    "y_pred_ada = best_ada.predict(X_test)\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "balanced_accuracy_ada = balanced_accuracy_score(y_test, y_pred_ada)\n",
    "f1_ada = f1_score(y_test, y_pred_ada)\n",
    "cm_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "\n",
    "print(\"\\nAdaBoost Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_ada)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_ada)\n",
    "print(\"F1 Score:\", f1_ada)\n",
    "print(\"Confusion Matrix:\\n\", cm_ada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-03-01 21:38:31,801] A new study created in memory with name: no-name-1cee67ed-2fb6-4827-906c-d21eccf5b066\n",
      "[I 2025-03-01 21:38:32,043] Trial 0 finished with value: 0.8304361075729794 and parameters: {'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 0 with value: 0.8304361075729794.\n",
      "[I 2025-03-01 21:38:32,338] Trial 1 finished with value: 0.8278715831252844 and parameters: {'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 1, 'criterion': 'entropy'}. Best is trial 0 with value: 0.8304361075729794.\n",
      "[I 2025-03-01 21:38:32,536] Trial 2 finished with value: 0.845334756289227 and parameters: {'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 1, 'criterion': 'gini'}. Best is trial 2 with value: 0.845334756289227.\n",
      "[I 2025-03-01 21:38:32,737] Trial 3 finished with value: 0.8553268223017125 and parameters: {'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 3 with value: 0.8553268223017125.\n",
      "[I 2025-03-01 21:38:33,020] Trial 4 finished with value: 0.8273791791905447 and parameters: {'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 1, 'criterion': 'entropy'}. Best is trial 3 with value: 0.8553268223017125.\n",
      "[I 2025-03-01 21:38:33,147] Trial 5 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:33,253] Trial 6 finished with value: 0.8497561527955344 and parameters: {'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:33,499] Trial 7 finished with value: 0.8340113078458795 and parameters: {'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:33,741] Trial 8 finished with value: 0.8342372973443795 and parameters: {'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 1, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:33,970] Trial 9 finished with value: 0.829077617662138 and parameters: {'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,077] Trial 10 finished with value: 0.8517510263667127 and parameters: {'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 3, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,250] Trial 11 finished with value: 0.8592181995899839 and parameters: {'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,412] Trial 12 finished with value: 0.855428957538735 and parameters: {'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,567] Trial 13 finished with value: 0.8586099365969815 and parameters: {'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,708] Trial 14 finished with value: 0.8607068579633662 and parameters: {'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 5, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,794] Trial 15 finished with value: 0.8453080958003019 and parameters: {'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:34,939] Trial 16 finished with value: 0.8602694503216519 and parameters: {'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,146] Trial 17 finished with value: 0.8481413180042793 and parameters: {'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,380] Trial 18 finished with value: 0.8406974975523601 and parameters: {'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 2, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,506] Trial 19 finished with value: 0.8606621214654899 and parameters: {'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,693] Trial 20 finished with value: 0.8498263669074342 and parameters: {'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,826] Trial 21 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:35,959] Trial 22 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,174] Trial 23 finished with value: 0.8401911914786971 and parameters: {'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,260] Trial 24 finished with value: 0.8453080958003019 and parameters: {'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,392] Trial 25 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,558] Trial 26 finished with value: 0.8544683813522898 and parameters: {'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 2, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,674] Trial 27 finished with value: 0.8606621214654899 and parameters: {'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:36,846] Trial 28 finished with value: 0.8575379198900613 and parameters: {'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,001] Trial 29 finished with value: 0.8599534915697665 and parameters: {'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,213] Trial 30 finished with value: 0.8382441189735264 and parameters: {'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 3, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,343] Trial 31 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,454] Trial 32 finished with value: 0.8606621214654899 and parameters: {'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,602] Trial 33 finished with value: 0.8595479072922769 and parameters: {'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,702] Trial 34 finished with value: 0.8517510263667127 and parameters: {'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:37,872] Trial 35 finished with value: 0.8575379198900613 and parameters: {'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,065] Trial 36 finished with value: 0.8498263669074342 and parameters: {'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,198] Trial 37 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,285] Trial 38 finished with value: 0.8452135778418898 and parameters: {'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,517] Trial 39 finished with value: 0.8327260102549252 and parameters: {'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,630] Trial 40 finished with value: 0.8606621214654899 and parameters: {'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,767] Trial 41 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:38,912] Trial 42 finished with value: 0.8599534915697665 and parameters: {'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,070] Trial 43 finished with value: 0.8563471506557352 and parameters: {'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,172] Trial 44 finished with value: 0.8517510263667127 and parameters: {'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,307] Trial 45 finished with value: 0.8608391293798603 and parameters: {'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,408] Trial 46 finished with value: 0.8517510263667127 and parameters: {'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,589] Trial 47 finished with value: 0.8559777957206117 and parameters: {'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'entropy'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,737] Trial 48 finished with value: 0.8599534915697665 and parameters: {'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,853] Trial 49 finished with value: 0.8606621214654899 and parameters: {'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 5 with value: 0.8608391293798603.\n",
      "[I 2025-03-01 21:38:39,854] A new study created in memory with name: no-name-4a46113b-1f91-40e6-8c13-29dc5fcc2248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Hyperparameters: {'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 5, 'criterion': 'gini'}\n",
      "Best CV Balanced Accuracy (Decision Tree): 0.8608391293798603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 21:38:43,514] Trial 0 finished with value: 0.8652592271966425 and parameters: {'n_estimators': 124, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.8652592271966425.\n",
      "[I 2025-03-01 21:38:50,071] Trial 1 finished with value: 0.867496179802734 and parameters: {'n_estimators': 211, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:08,932] Trial 2 finished with value: 0.8674555278362467 and parameters: {'n_estimators': 197, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:26,290] Trial 3 finished with value: 0.863903032249642 and parameters: {'n_estimators': 138, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:29,430] Trial 4 finished with value: 0.8546925817440714 and parameters: {'n_estimators': 84, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:31,130] Trial 5 finished with value: 0.8435247694812148 and parameters: {'n_estimators': 129, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:39,397] Trial 6 finished with value: 0.8633136416187692 and parameters: {'n_estimators': 236, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:40,395] Trial 7 finished with value: 0.849023339382941 and parameters: {'n_estimators': 57, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:44,062] Trial 8 finished with value: 0.8616944499041439 and parameters: {'n_estimators': 117, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:45,598] Trial 9 finished with value: 0.8632733905504676 and parameters: {'n_estimators': 60, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 1 with value: 0.867496179802734.\n",
      "[I 2025-03-01 21:39:54,333] Trial 10 finished with value: 0.8675314981651425 and parameters: {'n_estimators': 297, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 10 with value: 0.8675314981651425.\n",
      "[I 2025-03-01 21:40:02,780] Trial 11 finished with value: 0.8677756340932843 and parameters: {'n_estimators': 286, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:40:11,623] Trial 12 finished with value: 0.8670311795254964 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:40:20,910] Trial 13 finished with value: 0.8672592929045692 and parameters: {'n_estimators': 294, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:40:28,411] Trial 14 finished with value: 0.8672521078646993 and parameters: {'n_estimators': 254, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:40:51,582] Trial 15 finished with value: 0.8675262762490826 and parameters: {'n_estimators': 264, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:40:59,670] Trial 16 finished with value: 0.8668226562379772 and parameters: {'n_estimators': 277, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:41:06,610] Trial 17 finished with value: 0.8671256358110133 and parameters: {'n_estimators': 231, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:41:27,305] Trial 18 finished with value: 0.865258053006122 and parameters: {'n_estimators': 173, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:41:33,328] Trial 19 finished with value: 0.8674719189405753 and parameters: {'n_estimators': 173, 'max_depth': 18, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:41:39,517] Trial 20 finished with value: 0.8630299961562017 and parameters: {'n_estimators': 277, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:42:00,030] Trial 21 finished with value: 0.8676008448501864 and parameters: {'n_estimators': 261, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.8677756340932843.\n",
      "[I 2025-03-01 21:42:19,970] Trial 22 finished with value: 0.8679080015600327 and parameters: {'n_estimators': 247, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:42:37,304] Trial 23 finished with value: 0.866469929626269 and parameters: {'n_estimators': 244, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:42:53,510] Trial 24 finished with value: 0.8676244146065588 and parameters: {'n_estimators': 212, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:43:03,698] Trial 25 finished with value: 0.8605668067755626 and parameters: {'n_estimators': 216, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:43:22,993] Trial 26 finished with value: 0.8671209465216216 and parameters: {'n_estimators': 194, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:43:33,408] Trial 27 finished with value: 0.8666789254494619 and parameters: {'n_estimators': 155, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:43:57,485] Trial 28 finished with value: 0.865970673076158 and parameters: {'n_estimators': 216, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:44:03,695] Trial 29 finished with value: 0.8638552043819459 and parameters: {'n_estimators': 276, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:44:12,156] Trial 30 finished with value: 0.8546890323551024 and parameters: {'n_estimators': 228, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:44:32,010] Trial 31 finished with value: 0.8675574985129518 and parameters: {'n_estimators': 260, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:44:48,656] Trial 32 finished with value: 0.8666747430937602 and parameters: {'n_estimators': 248, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:45:08,667] Trial 33 finished with value: 0.8669714682501011 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:45:31,554] Trial 34 finished with value: 0.8673292174254795 and parameters: {'n_estimators': 270, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:45:47,839] Trial 35 finished with value: 0.8667449857418406 and parameters: {'n_estimators': 285, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 22 with value: 0.8679080015600327.\n",
      "[I 2025-03-01 21:46:02,297] Trial 36 finished with value: 0.8680734158613381 and parameters: {'n_estimators': 190, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:46:21,134] Trial 37 finished with value: 0.8669396652486856 and parameters: {'n_estimators': 190, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:46:24,916] Trial 38 finished with value: 0.8652032526031876 and parameters: {'n_estimators': 155, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:46:50,635] Trial 39 finished with value: 0.865951202320004 and parameters: {'n_estimators': 227, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:46:53,768] Trial 40 finished with value: 0.8425056247135574 and parameters: {'n_estimators': 206, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:47:13,018] Trial 41 finished with value: 0.8678766440815343 and parameters: {'n_estimators': 241, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:47:32,314] Trial 42 finished with value: 0.8678646552227983 and parameters: {'n_estimators': 246, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:47:46,666] Trial 43 finished with value: 0.8668628946146983 and parameters: {'n_estimators': 242, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:04,435] Trial 44 finished with value: 0.8677626123241376 and parameters: {'n_estimators': 184, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:11,487] Trial 45 finished with value: 0.86724241728445 and parameters: {'n_estimators': 89, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:17,564] Trial 46 finished with value: 0.8616079068924771 and parameters: {'n_estimators': 286, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:30,358] Trial 47 finished with value: 0.8608346537401868 and parameters: {'n_estimators': 255, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:37,478] Trial 48 finished with value: 0.8676063543945126 and parameters: {'n_estimators': 236, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:49,247] Trial 49 finished with value: 0.866104563187999 and parameters: {'n_estimators': 105, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 36 with value: 0.8680734158613381.\n",
      "[I 2025-03-01 21:48:49,248] A new study created in memory with name: no-name-45cba9c3-1e7e-4611-bacc-920595b334ee\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Hyperparameters: {'n_estimators': 190, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Best CV Balanced Accuracy (Random Forest): 0.8680734158613381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 21:48:50,867] Trial 0 finished with value: 0.8149035212510723 and parameters: {'n_estimators': 126, 'learning_rate': 0.021946989455914327, 'estimator__max_depth': 1, 'estimator__min_samples_split': 8}. Best is trial 0 with value: 0.8149035212510723.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:48:51,350] Trial 1 finished with value: 0.8529667748834291 and parameters: {'n_estimators': 145, 'learning_rate': 0.41931181602958334, 'estimator__max_depth': 3, 'estimator__min_samples_split': 9}. Best is trial 1 with value: 0.8529667748834291.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:48:51,902] Trial 2 finished with value: 0.8575071400059647 and parameters: {'n_estimators': 193, 'learning_rate': 0.6019350711072766, 'estimator__max_depth': 4, 'estimator__min_samples_split': 9}. Best is trial 2 with value: 0.8575071400059647.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:48:52,496] Trial 3 finished with value: 0.8163731707040697 and parameters: {'n_estimators': 141, 'learning_rate': 0.0682908295451555, 'estimator__max_depth': 1, 'estimator__min_samples_split': 8}. Best is trial 2 with value: 0.8575071400059647.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:48:58,515] Trial 4 finished with value: 0.8640041406260155 and parameters: {'n_estimators': 71, 'learning_rate': 0.014181516288995862, 'estimator__max_depth': 4, 'estimator__min_samples_split': 10}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:09,585] Trial 5 finished with value: 0.8626143612973962 and parameters: {'n_estimators': 182, 'learning_rate': 0.018484018831226127, 'estimator__max_depth': 4, 'estimator__min_samples_split': 4}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:11,662] Trial 6 finished with value: 0.8549537435634491 and parameters: {'n_estimators': 129, 'learning_rate': 0.07787277465430953, 'estimator__max_depth': 3, 'estimator__min_samples_split': 9}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:19,706] Trial 7 finished with value: 0.8630763080287529 and parameters: {'n_estimators': 139, 'learning_rate': 0.025270976537719352, 'estimator__max_depth': 4, 'estimator__min_samples_split': 8}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:21,637] Trial 8 finished with value: 0.8631551566383703 and parameters: {'n_estimators': 81, 'learning_rate': 0.10853826800168584, 'estimator__max_depth': 4, 'estimator__min_samples_split': 3}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:21,930] Trial 9 finished with value: 0.8410349991264983 and parameters: {'n_estimators': 166, 'learning_rate': 0.963599002496724, 'estimator__max_depth': 3, 'estimator__min_samples_split': 3}. Best is trial 4 with value: 0.8640041406260155.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:27,420] Trial 10 finished with value: 0.8649107956260877 and parameters: {'n_estimators': 54, 'learning_rate': 0.010613921074820011, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 10 with value: 0.8649107956260877.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:32,757] Trial 11 finished with value: 0.8627561932777453 and parameters: {'n_estimators': 53, 'learning_rate': 0.010068391517605477, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 10 with value: 0.8649107956260877.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:41,872] Trial 12 finished with value: 0.8651811764874745 and parameters: {'n_estimators': 87, 'learning_rate': 0.010184739607278174, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 12 with value: 0.8651811764874745.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:49,069] Trial 13 finished with value: 0.8656716424069154 and parameters: {'n_estimators': 90, 'learning_rate': 0.03539458686269232, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:55,133] Trial 14 finished with value: 0.8648288150323094 and parameters: {'n_estimators': 85, 'learning_rate': 0.04121508364735798, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:58,907] Trial 15 finished with value: 0.8654937794266283 and parameters: {'n_estimators': 104, 'learning_rate': 0.15496362719337178, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:49:59,443] Trial 16 finished with value: 0.8539531949887189 and parameters: {'n_estimators': 106, 'learning_rate': 0.21193606927928063, 'estimator__max_depth': 2, 'estimator__min_samples_split': 2}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:02,623] Trial 17 finished with value: 0.8647363185559568 and parameters: {'n_estimators': 104, 'learning_rate': 0.1997735200810839, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:04,885] Trial 18 finished with value: 0.8387098926273376 and parameters: {'n_estimators': 102, 'learning_rate': 0.04636665727001857, 'estimator__max_depth': 2, 'estimator__min_samples_split': 7}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:08,667] Trial 19 finished with value: 0.8650652624730378 and parameters: {'n_estimators': 110, 'learning_rate': 0.1390268919332382, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:12,597] Trial 20 finished with value: 0.8546954960047095 and parameters: {'n_estimators': 65, 'learning_rate': 0.03850537523717897, 'estimator__max_depth': 3, 'estimator__min_samples_split': 4}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:14,018] Trial 21 finished with value: 0.8638864933661005 and parameters: {'n_estimators': 89, 'learning_rate': 0.2932644767254988, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:21,606] Trial 22 finished with value: 0.8652572679957927 and parameters: {'n_estimators': 94, 'learning_rate': 0.03319796268480541, 'estimator__max_depth': 5, 'estimator__min_samples_split': 7}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:24,703] Trial 23 finished with value: 0.861086463969691 and parameters: {'n_estimators': 117, 'learning_rate': 0.06351051764541138, 'estimator__max_depth': 4, 'estimator__min_samples_split': 7}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:31,610] Trial 24 finished with value: 0.865530004451978 and parameters: {'n_estimators': 73, 'learning_rate': 0.0319807891959598, 'estimator__max_depth': 5, 'estimator__min_samples_split': 7}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:33,248] Trial 25 finished with value: 0.8621318339985912 and parameters: {'n_estimators': 68, 'learning_rate': 0.13591498296696336, 'estimator__max_depth': 4, 'estimator__min_samples_split': 4}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:37,732] Trial 26 finished with value: 0.8654429683909839 and parameters: {'n_estimators': 75, 'learning_rate': 0.05756881523215515, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:41,546] Trial 27 finished with value: 0.835946335138269 and parameters: {'n_estimators': 96, 'learning_rate': 0.026707779690258837, 'estimator__max_depth': 2, 'estimator__min_samples_split': 7}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:43,489] Trial 28 finished with value: 0.8645212916714492 and parameters: {'n_estimators': 64, 'learning_rate': 0.10395820672079324, 'estimator__max_depth': 4, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:50:55,150] Trial 29 finished with value: 0.8652853650762928 and parameters: {'n_estimators': 121, 'learning_rate': 0.017696517984531753, 'estimator__max_depth': 5, 'estimator__min_samples_split': 8}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:02,415] Trial 30 finished with value: 0.8651040319442108 and parameters: {'n_estimators': 77, 'learning_rate': 0.029621363397235565, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:07,294] Trial 31 finished with value: 0.8654149329833789 and parameters: {'n_estimators': 77, 'learning_rate': 0.051882323675137725, 'estimator__max_depth': 5, 'estimator__min_samples_split': 5}. Best is trial 13 with value: 0.8656716424069154.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:10,888] Trial 32 finished with value: 0.8662068223395597 and parameters: {'n_estimators': 94, 'learning_rate': 0.08160935044039067, 'estimator__max_depth': 5, 'estimator__min_samples_split': 4}. Best is trial 32 with value: 0.8662068223395597.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:12,028] Trial 33 finished with value: 0.8613514338463755 and parameters: {'n_estimators': 96, 'learning_rate': 0.17262941221596428, 'estimator__max_depth': 4, 'estimator__min_samples_split': 3}. Best is trial 32 with value: 0.8662068223395597.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:15,921] Trial 34 finished with value: 0.866368643405402 and parameters: {'n_estimators': 114, 'learning_rate': 0.07629317213683631, 'estimator__max_depth': 5, 'estimator__min_samples_split': 4}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:16,409] Trial 35 finished with value: 0.814908430843321 and parameters: {'n_estimators': 131, 'learning_rate': 0.0781283278225748, 'estimator__max_depth': 1, 'estimator__min_samples_split': 2}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:18,581] Trial 36 finished with value: 0.8626629834047037 and parameters: {'n_estimators': 114, 'learning_rate': 0.09223198927318464, 'estimator__max_depth': 4, 'estimator__min_samples_split': 4}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:31,379] Trial 37 finished with value: 0.8649980788518518 and parameters: {'n_estimators': 154, 'learning_rate': 0.019807328283844575, 'estimator__max_depth': 5, 'estimator__min_samples_split': 8}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:36,175] Trial 38 finished with value: 0.8639226145223138 and parameters: {'n_estimators': 60, 'learning_rate': 0.0357995191355563, 'estimator__max_depth': 4, 'estimator__min_samples_split': 10}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:38,712] Trial 39 finished with value: 0.8633322044317697 and parameters: {'n_estimators': 127, 'learning_rate': 0.07532517771678932, 'estimator__max_depth': 4, 'estimator__min_samples_split': 3}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:44,199] Trial 40 finished with value: 0.8652180574852828 and parameters: {'n_estimators': 136, 'learning_rate': 0.05141533352581878, 'estimator__max_depth': 5, 'estimator__min_samples_split': 9}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:45,237] Trial 41 finished with value: 0.8624673702823837 and parameters: {'n_estimators': 101, 'learning_rate': 0.3298392776542252, 'estimator__max_depth': 5, 'estimator__min_samples_split': 4}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:49,127] Trial 42 finished with value: 0.8653033390518337 and parameters: {'n_estimators': 111, 'learning_rate': 0.1314957588452389, 'estimator__max_depth': 5, 'estimator__min_samples_split': 4}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:51:58,062] Trial 43 finished with value: 0.8659784090899819 and parameters: {'n_estimators': 93, 'learning_rate': 0.02331114082003239, 'estimator__max_depth': 5, 'estimator__min_samples_split': 6}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:06,894] Trial 44 finished with value: 0.8652181724879471 and parameters: {'n_estimators': 91, 'learning_rate': 0.015288989581175836, 'estimator__max_depth': 5, 'estimator__min_samples_split': 7}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:13,553] Trial 45 finished with value: 0.8641432566643067 and parameters: {'n_estimators': 82, 'learning_rate': 0.023326454673117957, 'estimator__max_depth': 4, 'estimator__min_samples_split': 6}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:21,715] Trial 46 finished with value: 0.8660007873059818 and parameters: {'n_estimators': 84, 'learning_rate': 0.012820554573727608, 'estimator__max_depth': 5, 'estimator__min_samples_split': 8}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:31,330] Trial 47 finished with value: 0.8558492760550745 and parameters: {'n_estimators': 149, 'learning_rate': 0.013067567307900737, 'estimator__max_depth': 3, 'estimator__min_samples_split': 8}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:48,420] Trial 48 finished with value: 0.8653720389698533 and parameters: {'n_estimators': 192, 'learning_rate': 0.013426745106212454, 'estimator__max_depth': 5, 'estimator__min_samples_split': 3}. Best is trial 34 with value: 0.866368643405402.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10568\\1341510049.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
      "[I 2025-03-01 21:52:57,220] Trial 49 finished with value: 0.8635409846978075 and parameters: {'n_estimators': 122, 'learning_rate': 0.021327033942853426, 'estimator__max_depth': 4, 'estimator__min_samples_split': 6}. Best is trial 34 with value: 0.866368643405402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost Hyperparameters: {'n_estimators': 114, 'learning_rate': 0.07629317213683631, 'estimator__max_depth': 5, 'estimator__min_samples_split': 4}\n",
      "Best CV Balanced Accuracy (AdaBoost): 0.866368643405402\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# --- Assume X_train, y_train, w_train are already defined ---\n",
    "# Example: They come from your one-hot encoded data with sample weights\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Decision Tree Tuning with Optuna\n",
    "# ---------------------------\n",
    "def objective_dt(trial):\n",
    "    # Hyperparameter search space\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    clf = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 5-fold stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        w_tr = w_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "        preds = clf.predict(X_val)\n",
    "        scores.append(balanced_accuracy_score(y_val, preds))\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_dt = optuna.create_study(direction='maximize')\n",
    "study_dt.optimize(objective_dt, n_trials=50)\n",
    "print(\"Best Decision Tree Hyperparameters:\", study_dt.best_params)\n",
    "print(\"Best CV Balanced Accuracy (Decision Tree):\", study_dt.best_value)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Random Forest Tuning with Optuna\n",
    "# ---------------------------\n",
    "def objective_rf(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    \n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        oob_score=True\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        w_tr = w_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "        preds = clf.predict(X_val)\n",
    "        scores.append(balanced_accuracy_score(y_val, preds))\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "print(\"Best Random Forest Hyperparameters:\", study_rf.best_params)\n",
    "print(\"Best CV Balanced Accuracy (Random Forest):\", study_rf.best_value)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. AdaBoost Tuning with Optuna\n",
    "# ---------------------------\n",
    "def objective_ada(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "    # Tuning the base estimator's parameters\n",
    "    base_max_depth = trial.suggest_int('estimator__max_depth', 1, 5)\n",
    "    base_min_samples_split = trial.suggest_int('estimator__min_samples_split', 2, 10)\n",
    "    \n",
    "    base_est = DecisionTreeClassifier(\n",
    "        max_depth=base_max_depth,\n",
    "        min_samples_split=base_min_samples_split,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    clf = AdaBoostClassifier(\n",
    "        estimator=base_est,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        w_tr = w_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "        preds = clf.predict(X_val)\n",
    "        scores.append(balanced_accuracy_score(y_val, preds))\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_ada = optuna.create_study(direction='maximize')\n",
    "study_ada.optimize(objective_ada, n_trials=50)\n",
    "print(\"Best AdaBoost Hyperparameters:\", study_ada.best_params)\n",
    "print(\"Best CV Balanced Accuracy (AdaBoost):\", study_ada.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provided hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Test Set:\n",
      "Accuracy: 0.8696969696969697\n",
      "Balanced Accuracy: 0.863662796226804\n",
      "F1 Score: 0.9025463271563792\n",
      "Confusion Matrix:\n",
      " [[1494  269]\n",
      " [ 462 3385]]\n",
      "\n",
      "Random Forest Test Set:\n",
      "Accuracy: 0.8828877005347594\n",
      "Balanced Accuracy: 0.8682106896210571\n",
      "F1 Score: 0.9140164899882215\n",
      "Confusion Matrix:\n",
      " [[1461  302]\n",
      " [ 355 3492]]\n",
      "OOB Score: 0.8826291079812206\n",
      "\n",
      "AdaBoost Test Set:\n",
      "Accuracy: 0.8827094474153298\n",
      "Balanced Accuracy: 0.8647007244339313\n",
      "F1 Score: 0.9143675169182718\n",
      "Confusion Matrix:\n",
      " [[1439  324]\n",
      " [ 334 3513]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Define features including the module and separate target\n",
    "features = [\n",
    "    'score', 'Gender', 'Region', 'HLE', 'Age group',\n",
    "    'Credit Distribution', 'assessment_completion_ratio', 'total_vle_clicks',\n",
    "    'code_module'  # include module\n",
    "]\n",
    "\n",
    "X = df_final[features]\n",
    "y = df_final['target']\n",
    "\n",
    "# One-hot encode categorical variables (including code_module)\n",
    "X_encoded = pd.get_dummies(\n",
    "    X, \n",
    "    columns=['Gender', 'Region', 'HLE', 'Age group', 'code_module'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# Create Sample Weights by Module\n",
    "# -----------------------------------\n",
    "module_counts = df_final['code_module'].value_counts()\n",
    "df_final['module_weight'] = df_final['code_module'].map(lambda m: 1.0 / module_counts[m])\n",
    "sample_weights = df_final['module_weight']\n",
    "\n",
    "# -----------------------------------\n",
    "# Split the data into training and test sets\n",
    "# -----------------------------------\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X_encoded, y, sample_weights, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============\n",
    "# 1. Decision Tree using Optuna-optimized Hyperparameters\n",
    "# ============\n",
    "# Optimized Hyperparameters:\n",
    "#   max_depth = 6, min_samples_split = 9, min_samples_leaf = 5, criterion = 'gini'\n",
    "best_dt = DecisionTreeClassifier(\n",
    "    max_depth=6,\n",
    "    min_samples_split=9,\n",
    "    min_samples_leaf=5,\n",
    "    criterion='gini',\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "best_dt.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "balanced_accuracy_dt = balanced_accuracy_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_dt)\n",
    "print(\"F1 Score:\", f1_dt)\n",
    "print(\"Confusion Matrix:\\n\", cm_dt)\n",
    "\n",
    "# ============\n",
    "# 2. Random Forest using Optuna-optimized Hyperparameters\n",
    "# ============\n",
    "# Optimized Hyperparameters:\n",
    "#   n_estimators = 190, max_depth = 7, min_samples_split = 10, min_samples_leaf = 5, max_features = None\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=190,\n",
    "    max_depth=7,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=None,\n",
    "    bootstrap=True,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    oob_score=True\n",
    ")\n",
    "best_rf.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "balanced_accuracy_rf = balanced_accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "oob_score_rf = best_rf.oob_score_\n",
    "\n",
    "print(\"\\nRandom Forest Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n",
    "print(\"OOB Score:\", oob_score_rf)\n",
    "\n",
    "# ============\n",
    "# 3. AdaBoost using Optuna-optimized Hyperparameters\n",
    "# ============\n",
    "# Optimized Hyperparameters:\n",
    "#   n_estimators = 114, learning_rate = 0.07629317213683631,\n",
    "#   Base estimator: max_depth = 5, min_samples_split = 4\n",
    "base_est = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=4,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "best_ada = AdaBoostClassifier(\n",
    "    estimator=base_est,\n",
    "    n_estimators=114,\n",
    "    learning_rate=0.07629317213683631,\n",
    "    random_state=42\n",
    ")\n",
    "best_ada.fit(X_train, y_train, sample_weight=w_train)\n",
    "y_pred_ada = best_ada.predict(X_test)\n",
    "\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "balanced_accuracy_ada = balanced_accuracy_score(y_test, y_pred_ada)\n",
    "f1_ada = f1_score(y_test, y_pred_ada)\n",
    "cm_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "\n",
    "print(\"\\nAdaBoost Test Set:\")\n",
    "print(\"Accuracy:\", accuracy_ada)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_ada)\n",
    "print(\"F1 Score:\", f1_ada)\n",
    "print(\"Confusion Matrix:\\n\", cm_ada)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
